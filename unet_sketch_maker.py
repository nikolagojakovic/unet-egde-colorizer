# -*- coding: utf-8 -*-
"""unet-sketch-maker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aBiorvdV0WWaq5PNIOAo-M66FpvtSl9U
"""

# ============================================================
# 0. BASIC SETUP (GPU + LIBS)
# ============================================================
!nvidia-smi

!pip install -q kaggle opencv-python pillow torchvision torchmetrics gradio

import os, shutil, pathlib, cv2, numpy as np
from pathlib import Path
from PIL import Image

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)


# ============================================================
# 1. KAGGLE CREDENTIALS (EDIT THIS CELL ONLY)
# ============================================================

# >>>>> EDIT THESE TWO LINES WITH YOUR REAL CREDENTIALS <<<<<
KAGGLE_USERNAME = "x"   # e.g. "johnsmith"
KAGGLE_KEY = "x"             # e.g. ""

# DO NOT print them
os.makedirs("/root/.kaggle", exist_ok=True)
with open("/root/.kaggle/kaggle.json", "w") as f:
    f.write('{"username":"%s","key":"%s"}' % (KAGGLE_USERNAME, KAGGLE_KEY))
os.chmod("/root/.kaggle/kaggle.json", 0o600)

# Quick test: should list datasets without error
!kaggle datasets list | head


!kaggle datasets download -d badasstechie/celebahq-resized-256x256

# Check what was downloaded
!ls

# Unzip to /content/celebahq256
yes
N


# Inspect a few files
!ls celebahq256 | head

# 2) Clone APDrawingGAN2
# ============================================================
# 3. CLONE APDrawingGAN2 (PRETRAINED LINE-DRAWING MODEL)
# ============================================================

!pip install -q -r requirements.txt

# The authors provide a quick-start command using a pre-trained model.
# You will first use that on example data; then you can adapt for your faces.
# Command from the README (do NOT run yet):
# python test.py --dataroot dataset/test_single --name apdrawinggan++_author \
#   --model test --use_resnet --netG resnet_9blocks --which_epoch 150 \
#   --how_many 1000 --gpu_ids 0 --gpu_ids_p 0 --imagefolder images-single

# ============================================================
# PREP: build cartoon targets + line images (overwrite old ones)
# ============================================================
import shutil, cv2, numpy as np
from pathlib import Path

PHOTO_ROOT = Path("/content/celebahq256/celeba_hq_256")      # or your actual photo root
LINE_DIR   = Path("/content/data/lines_simple")
TARGET_DIR = Path("/content/data/colored_targets")

# Helper: list all jpg/png recursively
def list_images(root: Path):
    root = Path(root)
    imgs = sorted(list(root.rglob("*.jpg")) + list(root.rglob("*.png")))
    return imgs

# Clean old line/target folders and recreate
for d in [LINE_DIR, TARGET_DIR]:
    if d.exists():
        shutil.rmtree(d)
    d.mkdir(parents=True, exist_ok=True)

photo_paths = list_images(PHOTO_ROOT)
print("Total photos found:", len(photo_paths))

def make_cartoon_colored_sketch(img_bgr):
    # 1) Strong smoothing so colors become flat
    color = cv2.bilateralFilter(img_bgr, 9, 150, 150)
    color = cv2.bilateralFilter(color, 9, 150, 150)

    # 2) Edge detection (bold black lines)
    gray = cv2.cvtColor(color, cv2.COLOR_BGR2GRAY)
    gray_blur = cv2.medianBlur(gray, 7)
    edges = cv2.adaptiveThreshold(
        gray_blur, 255,
        cv2.ADAPTIVE_THRESH_MEAN_C,
        cv2.THRESH_BINARY,
        blockSize=9,
        C=2,
    )
    edges = cv2.bitwise_not(edges)  # black lines on white
    kernel = np.ones((2, 2), np.uint8)
    edges = cv2.dilate(edges, kernel, iterations=1)  # thicken lines
    edges_col = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)

    # 3) Color quantization (few flat colors)
    data = color.reshape((-1, 3)).astype(np.float32)
    K = 3  # fewer clusters => more cartoon‑like
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
    _, labels, centers = cv2.kmeans(
        data, K, None, criteria, 3, cv2.KMEANS_RANDOM_CENTERS
    )
    centers = np.uint8(centers)
    quant = centers[labels.flatten()].reshape(color.shape)

    # 4) Boost saturation & brightness slightly
    hsv = cv2.cvtColor(quant, cv2.COLOR_BGR2HSV)
    h, s, v = cv2.split(hsv)
    s = np.clip(s * 1.3, 0, 255).astype(np.uint8)
    v = np.clip(v * 1.05, 0, 255).astype(np.uint8)
    hsv = cv2.merge([h, s, v])
    quant_sat = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)

    # 5) Combine flat colors with bold edges
    cartoon = cv2.bitwise_and(quant_sat, edges_col)
    return cartoon, edges

max_images = 1500  # adjust for speed

count = 0
for img_path in photo_paths:
    img = cv2.imread(str(img_path))
    if img is None:
        continue

    cartoon, edges = make_cartoon_colored_sketch(img)

    # Save line image (3‑channel) for the model input
    edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
    cv2.imwrite(str(LINE_DIR / img_path.name), edges_rgb)

    # Save cartoon target
    cv2.imwrite(str(TARGET_DIR / img_path.name), cartoon)

    count += 1
    if count >= max_images:
        break

print("Prepared", count, "cartoon-style triplets")
print("Lines in:", LINE_DIR)
print("Targets in:", TARGET_DIR)

# ============================================================
# 4. DATASET + SIMPLE U-NET COLORIZER  (FIXED, SELF-CONTAINED)
# ============================================================

from pathlib import Path
from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
from PIL import Image

transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor()
])

# Helper: list all jpg/png recursively under a root
def list_images(root: Path):
    root = Path(root)
    imgs = sorted(list(root.rglob("*.jpg")) + list(root.rglob("*.png")))
    return imgs

class ColorizerDataset(Dataset):
    def __init__(self, photo_root, line_dir, target_dir, transform=None):
        self.photo_paths = list_images(Path(photo_root))
        self.line_dir = Path(line_dir)
        self.target_dir = Path(target_dir)
        self.transform = transform

    def __len__(self):
        return len(self.photo_paths)

    def __getitem__(self, idx):
        p = self.photo_paths[idx]
        name = p.name

        photo = Image.open(p).convert("RGB")
        line_path = self.line_dir / name
        target_path = self.target_dir / name

        # If line/target missing, fall back to photo to avoid crashes
        if not line_path.exists():
            line_path = p
        if not target_path.exists():
            target_path = p

        line = Image.open(line_path).convert("RGB")
        target = Image.open(target_path).convert("RGB")

        if self.transform:
            photo = self.transform(photo)
            line = self.transform(line)
            target = self.transform(target)

        x = torch.cat([photo, line], dim=0)  # 6 channels
        y = target
        return x, y

train_ds = ColorizerDataset(PHOTO_ROOT, LINE_DIR, TARGET_DIR, transform=transform)
print("Dataset size:", len(train_ds))
assert len(train_ds) > 0, "Dataset is empty. Check image paths and preprocessing."

train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2)

# ---------- U-Net definition ----------
class UNetBlock(nn.Module):
    def __init__(self, in_c, out_c):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_c, out_c, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_c, out_c, 3, padding=1),
            nn.ReLU(inplace=True),
        )
    def forward(self, x):
        return self.conv(x)

class SimpleUNet(nn.Module):
    def __init__(self, in_c=6, out_c=3):
        super().__init__()
        self.enc1 = UNetBlock(in_c, 64)
        self.pool1 = nn.MaxPool2d(2)
        self.enc2 = UNetBlock(64, 128)
        self.pool2 = nn.MaxPool2d(2)

        self.bottleneck = UNetBlock(128, 256)

        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = UNetBlock(256, 128)
        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = UNetBlock(128, 64)

        self.final = nn.Conv2d(64, out_c, 1)

    def forward(self, x):
        e1 = self.enc1(x)
        p1 = self.pool1(e1)
        e2 = self.enc2(p1)
        p2 = self.pool2(e2)

        b = self.bottleneck(p2)

        u2 = self.up2(b)
        d2 = self.dec2(torch.cat([u2, e2], dim=1))
        u1 = self.up1(d2)
        d1 = self.dec1(torch.cat([u1, e1], dim=1))

        out = self.final(d1)
        return out

model = SimpleUNet().to(device)
criterion = nn.L1Loss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

print("Model ready on", device)

# ============================================================
# 5. TRAIN THE COLORIZER (WITH PROGRESS)
# ============================================================

epochs = 2  # start smaller; you can increase later

for epoch in range(epochs):
    model.train()
    total_loss = 0.0

    print(f"\nEpoch {epoch+1}/{epochs}...")
    for batch_idx, (x, y) in enumerate(train_loader):
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        pred = model(x)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * x.size(0)

        # progress every 20 batches
        if (batch_idx + 1) % 20 == 0 or (batch_idx + 1) == len(train_loader):
            print(
                f"  batch {batch_idx+1:4d}/{len(train_loader):4d} "
                f"loss={loss.item():.4f}"
            )

    avg = total_loss / len(train_ds)
    print(f"Epoch {epoch+1} done - avg loss: {avg:.4f}")

save_path = "/content/colorizer_unet2.pth"
torch.save(model.state_dict(), save_path)
print("Saved model to", save_path)

# ============================================================
# 6. GRADIO APP: UPLOAD PHOTO -> LINES + COLORED SKETCH
# ============================================================
import gradio as gr

# Reload model cleanly (optional but good practice)
colorizer = SimpleUNet().to(device)
colorizer.load_state_dict(torch.load("/content/colorizer_unet2.pth", map_location=device))
colorizer.eval()

def line_and_colorize(photo):
    # 1) Line image: Canny
    img_bgr = cv2.cvtColor(np.array(photo), cv2.COLOR_RGB2BGR)
    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 80, 160)
    edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)

    # 2) Run colorizer
    photo_t = transform(photo).unsqueeze(0).to(device)
    line_t = transform(Image.fromarray(edges_rgb)).unsqueeze(0).to(device)
    x = torch.cat([photo_t, line_t], dim=1)

    with torch.no_grad():
        out = colorizer(x)[0].cpu()
    out_img = transforms.ToPILImage()(out.clamp(0, 1))

    line_img = Image.fromarray(edges_rgb)
    return line_img, out_img

demo = gr.Interface(
    fn=line_and_colorize,
    inputs=gr.Image(type="pil", label="Input photo"),
    outputs=[
        gr.Image(type="pil", label="Line (Canny)"),
        gr.Image(type="pil", label="Colored sketch"),
    ],
    title="Face Line Colorizer (Prototype)"
)

demo.launch()
